/*
 * SS Buglist and working progress:
 * - receiving data message destined for someone else (data_path_req)
 *   This was one of the first things I noticed when tracing multicast paths -- that sometimes
 *   data was being delivered to the wrong place.  At this point I didn't diagnose it -- just find its occurrence.
 *   This would have been present in MACEDON too.
 *
 * - GenericTreeMulticast doesn't let SS "forward" data if it's destined for the groupId rather than me.  Possible fix: use nexthop instead
 *   I had written generic tree multicasting code.  It would guard its forward handler with a check to make sure that
 *   the destination was the local machine (so upcalls in the middle of overlays would not be handled).  But this 
 *   guard prevented the multicasts from being received forwarded when being sent to the groupId.  This tended not
 *   to be a problem much of the time since much of the time the source was a member of a tree (and would therefore 
 *   forward by spreading it).  (The fix was to check if the next hop was me rather than the destination).
 *
 * - getParent returns NONE from client0 during temporary periods of no parent.  Fix: use avoidKids to avoid client0 being part of join anyway
 *   We later made a change to the API where if have no parent in a tree you are joined,
 *   a call to getParent returns the null MaceKey.  This caused some sends to fail (because it was relying on forwarding
 *   the multicast to the root via "getParent", which used to return the groupId if you were joined but with no parent).
 *   To fix this I set up scribe to avoid kids if you had called createGroup (instead of joinGroup).  This way, the source
 *   was only a member of the tree if it was the root of the tree.  This was better anyway, as we have (for a while now) 
 *   wanted a mechanism to do this without hard-coding into the services a way to tell if you were the source.  In any 
 *   case, this too is a Mace-only problem
 *
 * - Pushdown code was buggy -- pushing down kids on the wrong group -- causing too many kids
 *   Symptom was forwarding a multicast message to too many people.  Cause was a bug in the code where I was passing in 
 *   the wrong value for groupId on the pushdown call, resulting in too many kids.  (Mace-only)
 *
 * - splitstream not properly registered for verify handlers -- caused Pushdown bug a second time. (previously known but not mattered)
 *   I later changed splitstream code to make the join paths better by not temporarily accepting the child if we
 *   were going to immediately push it down.  But the way I did this exposed the fact that splitstream had not 
 *   properly set itself as the active join verifier with Scribe, and the code wasn't used.  Result -- too many kids again.
 *   (Mace-only)
 *
 * - Pastry forwarding was not properly passing in the hash id for next hop (previously known but not mattered)
 *   Looking at the nextHop in GenericTreeMulticast (or elsewhere) was wrong because the IPV4 address was being passed 
 *   in instead of the SHA160 address comparisons against the local address (for bug #2) then failed.
 *
 * - duplicate data delivers.
 *   Many multicast paths failed due to data being delivered to the same node twice.  Some were a result of 
 *   bad pastry forwarding, others are due to bad tree construction or tree changes.  Not fully diagnosed
 *   or fixed, but I am now very aware of it.
 *
 * - data messages sent and not received
 *   I then discovered that sometimes data sending from a node goes nowhere.  Given the ample spare bandwidth I was using
 *   at the time this shouldn't have happened. 
 *
 * - notifyIdSpaceChanged on a multicast path
 *   notifyIdSpaceChanged should never be called on a multicast path.  notifyIdSpaceChanged implies that 
 *   you just received a message from a node who when you add state for it which causes your id space 
 *   to change (receiving a multicast message from a node unknown to you but should be adjacent to you
 *   in your leafset).
 *   
 * - avoidKids bug: join was accepting a child when it had no state even if group was to avoid kids
 *   This was a bug in the avoidKids implementation -- the source was still accepting a child in the 
 *   case where it had no state.  I was storing avoidKids in the group state, but then purging it
 *   when I pushed the group away.  The fix was to keep a separate storage to track groups to avoid
 *   kids in.
 *
 * - mapping messages not sent when message not forwarded
 *   data_path_request messages are supposed to be responded to with mapping messages (for future
 *   direct routing).  But if a higher layer told pastry not to forward the message, it would also not
 *   return a mapping message, causing future routing to also be slow.
 *
 * - notifyIdSpaceChanged before update to address cache
 *   I was notifying that the id space changed before I updated the address cache.  As a result,
 *   direct routing which occured as a result of idspacechanged would fail to reach the proper destination,
 *   since usually the stale map entry pointed to me.  
 *
 * - mapping seems "out of date" -- added verify to help prevent this
 *   Still diagnosing problems with the mapping (it's actually been re-written now).  In the interim, I 
 *   added some code which updates the mappings with every node I learn about.  I never did figure out 
 *   what exactly happened here, but I completely re-wrote how this code worked since its the cause of 
 *   so many bugs.
 */
fragment invalidator TaskTooSlow {
  task(*) limit(REAL_TIME,{50ms+}) {
    any();
  }
}
fragment recognizer mcRouteDirect {
  notice ("data_::routeDirect::GenericTreeMulticast::data");
  task ("Pastry::routeDirect(Route)") { 
    task ("GenericTreeMulticast::forward(ReceiveDataHandler)") {
      task ("GenericTreeMulticast::forward::data(ReceiveDataHandler)");
    }
    xor {
branch:
      notice ("data_::route::Pastry::data");
      send(Receiver);
branch:
      notice ("data_::route::Pastry::data_path_req");
      send(Receiver);
    }
  }
}
fragment invalidator TooManyKids {
  task ("GenericTreeMulticast::deliver::data(ReceiveDataHandler)") {
    task ("SplitStreamMS::deliver(ReceiveDataHandler)") {
      task (*);
    }
    notice ("tree_::getParent");
    task ("ScribeMS::getParent(Tree)");
    repeat between 0 and 1 {
      call(mcRouteDirect);
    }
    notice ("tree_::getChildren");
    task ("ScribeMS::getChildren(Tree)");
    repeat between 19 and 10000000 {
      call(mcRouteDirect);
    }
  }
}
fragment recognizer mcGtmFwd {
  task ("GenericTreeMulticast::forward(ReceiveDataHandler)") {
    task ("GenericTreeMulticast::forward::data(ReceiveDataHandler)") {
      task ("SplitStreamMS::forward(ReceiveDataHandler)") {
        task ("SplitStreamMS::forward::Data(ReceiveDataHandler)");
      }
    }
  }
}
fragment recognizer mcGtmMcData {
  task ("SplitStreamMS::deliver(ReceiveDataHandler)") {
    task ("SplitStreamMS::deliver::Data(ReceiveDataHandler)");
  }
  notice ("tree_::getParent");
  task ("ScribeMS::getParent(Tree)");
  repeat between 0 and 1 {
    call (mcRouteDirect);
  }
  notice ("tree_::getChildren");
  task ("ScribeMS::getChildren(Tree)");
  repeat between 0 and 18 {
    call (mcRouteDirect);
  }
}
fragment recognizer mcGtmRcv {
  task ("GenericTreeMulticast::deliver(ReceiveDataHandler)") {
    task ("GenericTreeMulticast::deliver::data(ReceiveDataHandler)") {
      call(mcGtmMcData);
    }
  }
}
fragment recognizer updateState {
  maybe {
    notice ("control_::route::Pastry::inform_request");
    send(inform_request);
  }
}
fragment recognizer pastryPathReqFwd {
  recv(Sender_Receiver_Forwarder);
  task ("macedon_tcp_transport::deliver") {
    task ("Pastry::deliver(ReceiveDataHandler)") {
    xor {
branch:
      task ("Pastry::deliver::data_path_req(ReceiveDataHandler)") {
        call(updateState);
        notice ("data_::route::Pastry::data_path_req");
        send(Receiver_Forwarder);
      }
//branch: //This branch REALLY shouldn't happen!
 //     task ("Pastry::deliver::data(ReceiveDataHandler)") {
  //      call(updateState);
   //     task("GenericTreeMulticast::forward(ReceiveDataHandler)") {
    //      task("GenericTreeMulticast::forward::data(ReceiveDataHandler)");
     //   }
      //  notice ("data_::route::Pastry::data");
       // send(Receiver_Forwarder);
      //}
    }
    }
  }
}
fragment recognizer pastryRecvMapping {
  recv(mapping);
  task("macedon_tcp_transport::deliver") {
    task("Pastry::deliver(ReceiveDataHandler)") {
      task("Pastry::deliver::mapping(ReceiveDataHandler)");
    }
  }
}
fragment recognizer scribeForwardHBMsg {
  task("ScribeMS::forward(ReceiveDataHandler)") {
    task("ScribeMS::forward::Heartbeat(ReceiveDataHandler)");
  }
}
fragment recognizer scribeRecvHBMsg {
  task("ScribeMS::deliver(ReceiveDataHandler)") {
    task("ScribeMS::deliver::Heartbeat(ReceiveDataHandler)");
  }
}
fragment recognizer scribeRecvHeartbeat {
  recv(heartbeat);
  task("macedon_tcp_transport::deliver") {
    task("Pastry::deliver(ReceiveDataHandler)") {
      xor {
      branch:
        task("Pastry::deliver::data(ReceiveDataHandler)") {
          call(scribeForwardHBMsg);
          call(scribeRecvHBMsg);
        }
      branch:
        task("Pastry::deliver::data_path_req(ReceiveDataHandler)") {
          call(scribeForwardHBMsg);
          notice("control_::route::Pastry::mapping");
          send(myprince);
          call(scribeRecvHBMsg);
        }
      }
    }
  }
}
fragment recognizer scribeForwardPDMsg {
  task("ScribeMS::forward(ReceiveDataHandler)") {
    task("ScribeMS::forward::Pushdown(ReceiveDataHandler)");
  }
}
fragment recognizer scribeRecvPDMsg {
  task("ScribeMS::deliver(ReceiveDataHandler)") {
    task("ScribeMS::deliver::Pushdown(ReceiveDataHandler)") {
      xor {
      branch:
        task("SplitStreamMS::selectParent(ScribeTreeHandler)");
        notice("overlay_::routeDirect::ScribeMS::Join");
        task("Pastry::routeDirect(Route)") {
          task("ScribeMS::forward(ReceiveDataHandler)") {
            task("ScribeMS::forward::Join(ReceiveDataHandler)");
          }
          xor {
            branch: notice("data_::route::Pastry::data");
            branch: notice("data_::route::Pastry::data_path_req");
          }
          send(beneath);
        }
      branch:
        task("SplitStreamMS::selectParent(ScribeTreeHandler)") {
          notice("mc_::anycast::SplitStreamMS::JoinReq");
          task("GenericTreeMulticast::anycast(HierarchicalMulticast)") {
            notice("data_::routeDirect::GenericTreeMulticast::anycast_data");
            task("Pastry::routeDirect(Route)") {
              task("GenericTreeMulticast::forward(ReceiveDataHandler)") {
                task("GenericTreeMulticast::forward::anycast_data(ReceiveDataHandler)") {
                  notice("tree_::getChildren");
                  task("ScribeMS::getChildren(Tree)");
                  notice("data_::routeDirect::GenericTreeMulticast::anycast_data");
                  task("Pastry::routeDirect(Route)") {
                    task("GenericTreeMulticast::forward(ReceiveDataHandler)") {
                      task("GenericTreeMulticast::forward::anycast_data(ReceiveDataHandler)");
                    }
                    xor {
                    branch: notice("data_::route::Pastry::data");
                    branch: notice("data_::route::Pastry::data_path_req");
                    }
                    send(beneath);
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
fragment recognizer scribeRecvPushdown {
  recv(pushdown);
  task("macedon_tcp_transport::deliver") {
    task("Pastry::deliver(ReceiveDataHandler)") {
      xor {
      branch:
        task("Pastry::deliver::data(ReceiveDataHandler)") {
          call(scribeForwardPDMsg);
          call(scribeRecvPDMsg);
        }
      branch:
        task("Pastry::deliver::data_path_req(ReceiveDataHandler)") {
          call(scribeForwardPDMsg);
          notice("control_::route::Pastry::mapping");
          send(myprince);
          call(scribeRecvPDMsg);
        }
      }
    }
  }
}
recognizer LooseMcPath {
  level(3);
  thread Sender ("client0",1) {
    task ("SplitStreamMS::multicast(Multicast)") {
      notice ("mc_::multicast::SplitStreamMS::Data");
      task ("GenericTreeMulticast::multicast(HierarchicalMulticast)") {
        call(mcGtmMcData);
      }
    }
  }
  thread Receiver(!"client0",{0,99}) { //Truly it should be 99, but hey . . .
    repeat between 0 and 99 {
      call (pastryPathReqFwd);
    }
    maybe {
      recv(Sender_Receiver_Forwarder);
      task ("macedon_tcp_transport::deliver") {
        task ("Pastry::deliver(ReceiveDataHandler)") {
          xor { 
branch:
            task ("Pastry::deliver::data(ReceiveDataHandler)") {
              call (updateState);
              call (mcGtmFwd);
              call (mcGtmRcv);
            }
branch:
            task ("Pastry::deliver::data_path_req(ReceiveDataHandler)") {
              call (updateState); //once for the 1-hop sender, once for the origin
              call (updateState); //once for the 1-hop sender, once for the origin
              call (mcGtmFwd);
              notice("control_::route::Pastry::mapping"); 
              send(Sender); //Mapping message
              call (mcGtmRcv);
            }
          }
        }
      }
    }
    repeat between 0 and 99 {
      xor {
      branch:
      recv(Sender_Receiver_Forwarder);
      task ("macedon_tcp_transport::deliver") {
        task ("Pastry::deliver(ReceiveDataHandler)") {
          xor { 
          branch:
            task ("Pastry::deliver::data(ReceiveDataHandler)") {
              call (updateState);
              call (mcGtmFwd);
            }
          branch:
            task ("Pastry::deliver::data_path_req(ReceiveDataHandler)") {
              call (updateState); //once for the 1-hop sender, once for the origin
              call (updateState); //once for the 1-hop sender, once for the origin
              call (mcGtmFwd);
              notice("control_::route::Pastry::mapping"); 
              send(Sender); //Mapping message
            }
          }
        }
      }
      branch:
      call (pastryPathReqFwd);
      }
    }
  }
  thread RecvMapping(*,{0,100}) {
    repeat between 1 and 18 {
      call (pastryRecvMapping);
    }
  }
}
recognizer McPath {
  level(3);
  thread Sender ("client0",1) {
    task ("SplitStreamMS::multicast(Multicast)") {
      notice ("mc_::multicast::SplitStreamMS::Data");
      task ("GenericTreeMulticast::multicast(HierarchicalMulticast)") {
        call(mcGtmMcData);
      }
    }
  }
  thread Receiver(!"client0",{0,99}) { //Truly it should be 99, but hey . . .
    repeat between 0 and 99 {
      call (pastryPathReqFwd);
    }
    maybe {
      recv(Sender_Receiver_Forwarder);
      task ("macedon_tcp_transport::deliver") {
        task ("Pastry::deliver(ReceiveDataHandler)") {
          xor { 
branch:
            task ("Pastry::deliver::data(ReceiveDataHandler)") {
              call (updateState);
              call (mcGtmFwd);
              call (mcGtmRcv);
            }
branch:
            task ("Pastry::deliver::data_path_req(ReceiveDataHandler)") {
              call (updateState); //once for the 1-hop sender, once for the origin
              call (updateState); //once for the 1-hop sender, once for the origin
              call (mcGtmFwd);
              notice("control_::route::Pastry::mapping"); 
              send(Sender); //Mapping message
              call (mcGtmRcv);
            }
          }
        }
      }
    }
    repeat between 0 and 99 {
      call (pastryPathReqFwd);
    }
  }
  thread RecvMapping(*,{0,100}) {
    repeat between 1 and 18 {
      call (pastryRecvMapping);
    }
  }
}
validator ValidMcPath {
  level(3);
  thread Sender ("client0",1) {
    task ("SplitStreamMS::multicast(Multicast)") {
      notice ("mc_::multicast::SplitStreamMS::Data");
      task ("GenericTreeMulticast::multicast(HierarchicalMulticast)") {
        call(mcGtmMcData);
      }
    }
  }
  thread Receiver(!"client0",99) { 
    repeat between 0 and 99 {
      call (pastryPathReqFwd);
    }
    recv(Sender_Receiver_Forwarder);
    task ("macedon_tcp_transport::deliver") {
      task ("Pastry::deliver(ReceiveDataHandler)") {
        xor { 
branch:
          task ("Pastry::deliver::data(ReceiveDataHandler)") {
            call (updateState);
            call (mcGtmFwd);
            call (mcGtmRcv);
          }
branch:
          task ("Pastry::deliver::data_path_req(ReceiveDataHandler)") {
            call (updateState); //once for the 1-hop sender, once for the origin
            call (updateState); //once for the 1-hop sender, once for the origin
            call (mcGtmFwd);
            notice("control_::route::Pastry::mapping"); 
            send(Sender); //Mapping message
            call (mcGtmRcv);
          }
        }
      }
    }
    repeat between 0 and 99 {
      call (pastryPathReqFwd);
    }
  }
  thread RecvMapping(*,{0,100}) {
    repeat between 1 and 18 {
      call (pastryRecvMapping);
    }
  }
}
recognizer dud {
  level(3);
  thread dude (*,1) {
    any();
  }
  thread dude2 (*,{0,10000}) {
    recv(foo);
    any();
  }
}
validator PMaceInit {
  level(3);
  thread dude (*,1) {
    task("Pastry::maceInit(Route)") {
      notice("control_::getLocalAddress");
    }
  }
}
validator GtmMaceInit {
  level(3);
  thread dude (*,1) {
    task("GenericTreeMulticast::maceInit(HierarchicalMulticast)") {
      notice("data_::getLocalAddress");
      task("Pastry::getLocalAddress(Route)");
      notice("tree_::isJoinedOverlay");
      task("ScribeMS::getOverlayJoinStatus(Tree)") {
        notice("overlay_::getOverlayJoinStatus");
        task("Pastry::getOverlayJoinStatus(OverlayRoute)");
      }
    }
  }
}
fragment validator NetError { //This will eventually be more complex
  task("Pastry::error(NetworkErrorHandler)");
}
fragment invalidator SJoinGAnyDeliver {
  task("SplitStreamMS::deliver::JoinReq(ReceiveDataHandler)");
}
validator SsMaceInitLocal { 
  level(3);
  limit(HOSTS, {=1});
  thread dude(*,1) {
    task("SplitStreamMS::maceInit(Multicast)") {
      notice("tree_::getLocalAddress");
      task("ScribeMS::getLocalAddress(Tree)");
      notice("tree_::setAuthoritativeGroupJoinHandler");
      task("ScribeMS::setAuthoritativeGroupJoinHandler(ScribeTree)");
      send(printer_timer);
    }
  }
  thread printer(*,1) { //works only until the scheduler thread changes
    repeat between 0 and 100000 {
      recv(printer_timer);
      task("SplitStreamMS::timer printer") {
        send(printer_timer);
      }
    }
    repeat between 0 and 1 {
      recv(printer_timer);
      task("SplitStreamMS::timer printer");
    }
  }
}
validator PPrinter {
  level(3);
  limit(HOSTS, {=1});
  thread dude(*,1) {
    send(printer_timer);
  }
  thread printer(*,1) { //works only until the scheduler thread changes
    repeat between 0 and 100000 {
      recv(printer_timer);
      task("Pastry::timer printer") {
        send(printer_timer);
      }
    }
    maybe {
      recv(printer_timer);
      task("Pastry::timer printer");
    }
  }
}
validator SPrinter {
  level(3);
  limit(HOSTS, {=1});
  thread dude(*,1) {
    send(printer_timer);
  }
  thread printer(*,1) { //works only until the scheduler thread changes
    repeat between 0 and 100000 {
      recv(printer_timer);
      task("ScribeMS::timer printer") {
        send(printer_timer);
      }
    }
    maybe {
      recv(printer_timer);
      task("ScribeMS::timer printer");
    }
  }
}
recognizer PProbe {
  limit(HOSTS, {1,2});
  thread prober(*,1) {
    notice("UDP::route::Pastry::probe");
    maybe {
      send(probee);
    }
  }
  thread probee(*,{0,1}) {
    recv(prober);
    task("macedon_udp_transport::deliver") {
      task("Pastry::deliver(ReceiveDataHandler)") {
        task("Pastry::deliver::probe(ReceiveDataHandler)") {
          notice("UDP::route::Pastry::probe_reply");
          maybe {
            send(probee2);
          }
        }
      }
    }
  }
  thread probee2(*,{0,1}) {
    recv(probee);
    task("macedon_udp_transport::deliver") {
      task("Pastry::deliver(ReceiveDataHandler)") {
        task("Pastry::deliver::probe_reply(ReceiveDataHandler)");
      }
    }
  }
}
validator PValidProbe {
  limit(HOSTS, {=2});
  thread prober(*,1) {
    notice("UDP::route::Pastry::probe");
    send(probee);
  }
  thread probee(*,1) {
    recv(prober);
    task("macedon_udp_transport::deliver") {
      task("Pastry::deliver(ReceiveDataHandler)") {
        task("Pastry::deliver::probe(ReceiveDataHandler)") {
          notice("UDP::route::Pastry::probe_reply");
          send(probee2);
        }
      }
    }
  }
  thread probee2(*,1) {
    recv(probee);
    task("macedon_udp_transport::deliver") {
      task("Pastry::deliver(ReceiveDataHandler)") {
        task("Pastry::deliver::probe_reply(ReceiveDataHandler)");
      }
    }
  }
}
validator SCreateGroup {
  thread client0app("client0",1) {
    notice("downcall createGroup");
    notice("overlay_::route::ScribeMS::CreateGroup");
    task("Pastry::route(Route)") {
      task("ScribeMS::forward(ReceiveDataHandler)") {
        task("ScribeMS::forward::CreateGroup(ReceiveDataHandler)");
      }
      task("ScribeMS::deliver(ReceiveDataHandler)") {
        task("ScribeMS::deliver::CreateGroup(ReceiveDataHandler)");
      }
    }
    send(client0data);
  }
  thread client0data("client0",1) {
    recv(client0app);
    notice("notifyIdSpaceChanged-avoidKids");
    notice("overlay_::route::ScribeMS::CreateGroup");
    task("Pastry::route(Route)") {
      task("ScribeMS::forward(ReceiveDataHandler)") {
        task("ScribeMS::forward::CreateGroup(ReceiveDataHandler)");
      }
      notice("data_::route::Pastry::data");
      send(root);
    }
    maybe { //XXX: thread pools would be REALLY NICE!
      recv(client0data);
      task("macedon_tcp_transport::deliver") {
        task("Pastry::deliver(ReceiveDataHandler)") {
          task("Pastry::deliver::no_match(ReceiveDataHandler)") {
          }
        }
      }
    }
  }
  thread client0other("client0",{0,1}) {
    recv(client0data);
    task("macedon_tcp_transport::deliver") {
      task("Pastry::deliver(ReceiveDataHandler)") {
        task("Pastry::deliver::no_match(ReceiveDataHandler)") {
        }
      }
    }
  }
  thread nonroot(!"client0",{0,5}) {
    recv(client0data);
    task("macedon_tcp_transport::deliver") {
      task("Pastry::deliver(ReceiveDataHandler)") {
        task("Pastry::deliver::data(ReceiveDataHandler)") {
          task("ScribeMS::forward(ReceiveDataHandler)") {
            task("ScribeMS::forward::CreateGroup(ReceiveDataHandler)");
          }
          notice("data_::route::Pastry::data");
          send(client0data, root);
        }
      }
    }
  }
  thread root(!"client0",1) {
    recv(client0data,nonroot);
    task("macedon_tcp_transport::deliver") {
      task("Pastry::deliver(ReceiveDataHandler)") {
        task("Pastry::deliver::data(ReceiveDataHandler)") {
          task("ScribeMS::forward(ReceiveDataHandler)") {
            task("ScribeMS::forward::CreateGroup(ReceiveDataHandler)");
          }
          task("ScribeMS::deliver(ReceiveDataHandler)") {
            task("ScribeMS::deliver::CreateGroup(ReceiveDataHandler)");
          }
          maybe {
            notice("control_::route::Pastry::no_match");
            send(client0data);
          }
        }
      }
    }
  }
}
validator SsJoinGroup {
  thread nonsource(!"client0",1) {
    task("SplitStreamMS::joinGroup(Group)") {
      repeat 17 {
        notice("tree_::joinGroup");
        task("ScribeMS::joinGroup(ScribeTree)");
      }
    }
  }
}
validator SsCreateGroup {
  thread nonsource("client0",1) {
    task("SplitStreamMS::createGroup(Group)") {
      repeat 16 {
        notice("tree_::createGroup");
        task("ScribeMS::createGroup(ScribeTree)");
      }
    }
  }
}
fragment recognizer computeOutdegree {
  repeat 16 {
    notice("tree_::getChildren");
    task("ScribeMS::getChildren(Tree)");
  }
}
fragment recognizer selectPushdownChild {
  repeat 16 {
    notice("tree_::isRoot");
    task("ScribeMS::isRoot(Tree)");
    notice("tree_::getChildren");
    task("ScribeMS::getChildren(Tree)");
  }
  maybe {
    notice("tree_::pushdownChild");
    task("ScribeMS::pushdownChild(ScribeTree)");
  }
}
validator SJoinGroupRoot {
  limit(HOSTS, {=1});
  thread appJoin(!"client0",1) {
    notice("downcall joinGroup - am root");
    send(timer);
  }
  thread timer(!"client0",1) {
    repeat between 0 and 1000000 {
      recv(appJoin,timer);
      notice("hbTimer");
      send(timer);
    }
    recv(appJoin,timer);
    notice("hbTimer");
  }
}
fragment recognizer sendHeartbeat {
  notice("overlay_::routeDirect::ScribeMS::Heartbeat");
  task("Pastry::routeDirect(Route)") {
    task("ScribeMS::forward(ReceiveDataHandler)") {
      task("ScribeMS::forward::Heartbeat(ReceiveDataHandler)");
    }
    xor {
    branch: notice("data_::route::Pastry::data_path_req");
    branch: notice("data_::route::Pastry::data");
    }
    send(dudder);
  }
}
fragment recognizer joinOtherNodeMsg {
  maybe {
    task("ScribeMS::notifyIdSpaceChanged(OverlayStructureHandler)");
  }
  xor {
  branch:
    task("ScribeMS::forward(ReceiveDataHandler)") {
      xor {
      branch:
        task("ScribeMS::forward::Join(ReceiveDataHandler)") {
          task("SplitStreamMS::verifyJoinGroup(GroupJoinHandler)") {
            call(computeOutdegree);
            maybe {
              call(selectPushdownChild);
            }
          }
          xor {
          branch:
            call(sendHeartbeat);
            task("SplitStreamMS::notifyChildAdded(TreeStructureHandler)") {
              call(computeOutdegree);
              maybe {
                notice("tree_::leaveGroup");
                task("ScribeMS::leaveGroup(ScribeTree)");
              }
            }
            send(somewhere);
          branch:
            notice("overlay_::route::ScribeMS::Pushdown");
            task("Pastry::route(Route)") {
              task("ScribeMS::forward(ReceiveDataHandler)") {
                task("ScribeMS::forward::Pushdown(ReceiveDataHandler)");
              }
              notice("data_::route::Pastry::data");
              send(lullaby);
            }
          }
        }
      }
    }
  branch:
    task("GenericTreeMulticast::forward(ReceiveDataHandler)") {
      task("GenericTreeMulticast::forward::anycast_data(ReceiveDataHandler)") {
        notice("tree_::getChildren");
        task("ScribeMS::getChildren(Tree)");
        notice("data_::routeDirect::GenericTreeMulticast::anycast_data");
        task("Pastry::routeDirect(Route)") {
          task("GenericTreeMulticast::forward(ReceiveDataHandler)") {
            task("GenericTreeMulticast::forward::anycast_data(ReceiveDataHandler)");
          }
          xor {
          branch: notice("data_::route::Pastry::data_path_req");
          branch: notice("data_::route::Pastry::data");
          }
          send(foo);
        }
      }
    }
  }
  maybe {
          notice("control_::route::Pastry::mapping");
          send(myprince);
  }
}
fragment recognizer scribePushdownForwarder {
  recv(foobar);
  task("macedon_tcp_transport::deliver") {
    task("Pastry::deliver(ReceiveDataHandler)") {
      task("Pastry::deliver::data(ReceiveDataHandler)") {
        task("ScribeMS::forward(ReceiveDataHandler)") {
          task("ScribeMS::forward::Pushdown(ReceiveDataHandler)");
        }
        notice("data_::route::Pastry::data");
        send(baz);
      }
    }
  }
}
recognizer SJoinGroup {
  thread appJoin(!"client0",1) {
    notice("downcall joinGroup - am root");
    send(joiner);
  }
  thread joiner(!"client0",1) {
    recv(appJoin); //This should be on the same host.
    notice("notifyIdSpaceChanged-joining"); //NOTE: in the future this could be different.
    notice("overlay_::routeDirect::ScribeMS::Join");
    task("Pastry::routeDirect(Route)") {
      task("ScribeMS::forward(ReceiveDataHandler)") {
        task("ScribeMS::forward::Join(ReceiveDataHandler)");
      }
      xor {
      branch:
      notice("data_::route::Pastry::data_path_req");
      branch:
      notice("data_::route::Pastry::data");
      }
      send(hmm);
    }
    send(joinercont);
    xor { //either the pastry control thread or the scribe data thread
      branch:
      repeat between 0 and 100 {
        call(pastryRecvMapping);
      }
      branch:
      repeat between 0 and 100 {
        call(scribeRecvHeartbeat);
      }
    }
  }
  thread joinerHbRcv(!"client0",{0,1}) {
    repeat between 1 and 100 {
      xor {
      branch:
        call(scribeRecvHeartbeat);
      branch:
        call(scribeRecvPushdown);
      }
    }
  }
  thread joinerTimer(!"client0",1) { //Note: this is a recognizer only -- not concerned about the number of times things happen.
    repeat between 1 and 1000 {
      recv(joiner,joinerTimer,joinerroot);
      notice("hbTimer");
      maybe {
        xor {
        branch:
          notice("overlay_::routeDirect::ScribeMS::Join");
          task("Pastry::routeDirect(Route)") {
            task("ScribeMS::forward(ReceiveDataHandler)") {
              task("ScribeMS::forward::Join(ReceiveDataHandler)");
            }
            xor {
            branch:
              notice("data_::route::Pastry::data_path_req");
            branch:
              notice("data_::route::Pastry::data");
            }
            send(elsewhere);
          }
        branch:
          notice("overlay_::route::ScribeMS::Join"); //The route is when the timeout has happened and am rejoining
          task("Pastry::route(Route)") {
            task("ScribeMS::forward(ReceiveDataHandler)") {
              task("ScribeMS::forward::Join(ReceiveDataHandler)");
            }
            notice("data_::route::Pastry::data");
            send(elsewhere);
          }
        }
      }
      maybe {
        send(joiner,joinerTimer,joinerroot);
      }
    }
  }
  thread dude(*,{0,10000}) {
    recv(dudette);
    task("macedon_tcp_transport::deliver") {
      task("Pastry::deliver(ReceiveDataHandler)") {
        xor {
        branch:
          task("Pastry::deliver::data_path_req(ReceiveDataHandler)") {
            call(joinOtherNodeMsg);
          }
        branch:
          task("Pastry::deliver::data(ReceiveDataHandler)") {
            call(joinOtherNodeMsg);
          }
        }
      }
    }
    any(); //downcall pushdownChild
  }
  thread RecvMapping(*,{0,100}) {
    repeat between 1 and 18 {
      call (pastryRecvMapping);
    }
  }
  thread pprf(*,{0,100}) {
    repeat between 0 and 18 {
      xor {
      branch: call(pastryPathReqFwd);
      branch: call(scribePushdownForwarder);
      }
    }
  }
  thread client0node("client0",{0,1}) {
    repeat between 0 and 18 {
      call (pastryRecvMapping);
    }
    recv(rainbow);
    notice("notifyIdSpaceChanged-avoidKids");
    notice("overlay_::routeDirect::ScribeMS::Pushdown");
    task("Pastry::routeDirect(Route)") {
      task("ScribeMS::forward(ReceiveDataHandler)") {
        task("ScribeMS::forward::Pushdown(ReceiveDataHandler)");
      }
      xor {
        branch:
          notice("data_::route::Pastry::data_path_req");
        branch:
          notice("data_::route::Pastry::data");
      }
      send(over);
    }
    repeat between 0 and 18 {
      call (pastryRecvMapping);
    }
  }
  thread parentHBTimer(*,{1,5}) {
    repeat between 0 and 40 {
      recv(elsewhere);
      notice("hbTimer-parent");
      call(sendHeartbeat);
      maybe{
        send(self);
      }
    }
  }
}
assert(instances(SCreateGroup)==16);
assert(instances(SsCreateGroup)==1);
assert(instances(SsJoinGroup)==99);
assert(instances(SsMaceInitLocal)==100);
assert(instances(PMaceInit)==100);
assert(instances(PPrinter)==100);
assert(instances(GtmMaceInit)==100);
assert(instances(SJoinGroupRoot)==16);
